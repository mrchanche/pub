{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "# Func to free up XPU VRAM from allocator\n",
    "def clearvram():\n",
    "    torch.xpu.memory.empty_cache()\n",
    "\n",
    "# Clear VRAM\n",
    "clearvram()\n",
    "\n",
    "# Load a pre-trained model (e.g., \"base\", \"small\", \"medium\", \"large\")\n",
    "model = whisper.load_model(\"turbo\", device=\"xpu\") \n",
    "\n",
    "# Transcribe your audio file\n",
    "result = model.transcribe(\"test.wav\")\n",
    "\n",
    "print(result[\"text\"])\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" # Replace with your chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"xpu\", torch_dtype=torch.bfloat16) # Adjust dtype as needed\n",
    "\n",
    "prompt = result[\"text\"]\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"xpu\")\n",
    "\n",
    "# Generate text\n",
    "output_ids = model.generate(inputs.input_ids, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "# Clear VRAM\n",
    "clearvram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b267f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Func to free up XPU VRAM from allocator\n",
    "def clearvram():\n",
    "    torch.xpu.memory.empty_cache()\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Clear VRAM\n",
    "clearvram()\n",
    "\n",
    "# Load a pre-trained model (e.g., \"base\", \"small\", \"medium\", \"large\")\n",
    "model = whisper.load_model(\"turbo\", device=\"cpu\") \n",
    "\n",
    "# Transcribe your audio file\n",
    "result = model.transcribe(\"test.wav\")\n",
    "\n",
    "print(result[\"text\"])\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" # Replace with your chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16) # Adjust dtype as needed\n",
    "\n",
    "prompt = result[\"text\"]\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Generate text\n",
    "output_ids = model.generate(inputs.input_ids, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85657e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  Online accounts. Supposedly the night of the Trump call, we did a show in the evening. At that time, we used to record like at the end of the workday, like at five o'clock or so. The problem was John was already drunk. So we switched to 11 a.m. So we got hung over John instead of drunk John. So they would record at five o'clock and John would be shit face. Because if you remember back then, he was inviting and challenging people and saying, I'm at Pickwick Pub every day at three o'clock. Come come fight me. He wasn't lying. Yeah. But the night of the Trump call, we did an entire show. Somebody didn't show up. I don't know if it was. I really don't know. An ex stern guy. Grillo is the only one that comes to mind. And I apologize if I'm wrong. Sorry. Somebody else. But just in back and forth conversation, Royce says, you know, my cousin, Vinny thing, Jerry Callow, Jerry Gallo, John Melendez, Senator Menendez. So I'm sitting up at the board. So to the White House, we call. And it's fine with him, we call. Just fish and jelly. And I'm sitting up at the board. And let's talk to him. And we call the White House. And we call. And we call. And we call. And we call.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.31s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Prompt:\n",
      " <|system|>\n",
      "You are a helpful assistant that summarizes podcast transcripts concisely.<|end|>\n",
      "<|user|>\n",
      "Please summarize the following podcast excerpt in 2-3 sentences:\n",
      "\n",
      " Online accounts. Supposedly the night of the Trump call, we did a show in the evening. At that time, we used to record like at the end of the workday, like at five o'clock or so. The problem was John was already drunk. So we switched to 11 a.m. So we got hung over John instead of drunk John. So they would record at five o'clock and John would be shit face. Because if you remember back then, he was inviting and challenging people and saying, I'm at Pickwick Pub every day at three o'clock. Come come fight me. He wasn't lying. Yeah. But the night of the Trump call, we did an entire show. Somebody didn't show up. I don't know if it was. I really don't know. An ex stern guy. Grillo is the only one that comes to mind. And I apologize if I'm wrong. Sorry. Somebody else. But just in back and forth conversation, Royce says, you know, my cousin, Vinny thing, Jerry Callow, Jerry Gallo, John Melendez, Senator Menendez. So I'm sitting up at the board. So to the White House, we call. And it's fine with him, we call. Just fish and jelly. And I'm sitting up at the board. And let's talk to him. And we call the White House. And we call. And we call. And we call. And we call.<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "Summary:\n",
      " he Trump call, the hosts recorded a show at 11 a.m. instead of their usual 5 p.m. time, due to John's hangover. They engaged in discussions with Royce, mentioning various individuals, including Senator Menendez, while also making multiple attempts to reach the White House for a conversation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import whisper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Func to free up XPU VRAM from allocator\n",
    "def clearvram():\n",
    "    torch.xpu.memory.empty_cache()\n",
    "\n",
    "# Clear VRAM\n",
    "clearvram()\n",
    "\n",
    "# Load a pre-trained model (e.g., \"base\", \"small\", \"medium\", \"large\")\n",
    "whisper_model = whisper.load_model(\"turbo\", device=\"xpu\")\n",
    "result = whisper_model.transcribe(\"test2.wav\")\n",
    "transcript = result[\"text\"]\n",
    "print(\"Transcript:\", transcript)\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" # Replace with your chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"xpu\", torch_dtype=torch.bfloat16) # Adjust dtype as needed\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# === 3. Build PROPER chat prompt for Phi-3 ===\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes podcast transcripts concisely.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Please summarize the following podcast excerpt in 2-3 sentences:\\n\\n{transcript}\"}\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True  # This adds <|assistant|>\n",
    ")\n",
    "\n",
    "print(\"\\nFormatted Prompt:\\n\", prompt)\n",
    "\n",
    "# === 4. Tokenize and generate ===\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"xpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# === 5. Decode only the NEW part (after input) ===\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):].strip()  # Remove input prompt\n",
    "\n",
    "print(\"\\nSummary:\\n\", response)\n",
    "\n",
    "# === Cleanup ===\n",
    "clearvram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8eb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
